1.  Before I changed the environment variable, `locale` command has the following output.
      LANG=en_US.UTF-8
      LC_CTYPE="en_US.UTF-8"
      LC_NUMERIC="en_US.UTF-8"
      LC_TIME="en_US.UTF-8"
      LC_COLLATE="en_US.UTF-8"
      LC_MONETARY="en_US.UTF-8"
      LC_MESSAGES="en_US.UTF-8"
      LC_PAPER="en_US.UTF-8"
      LC_NAME="en_US.UTF-8"
      LC_ADDRESS="en_US.UTF-8"
      LC_TELEPHONE="en_US.UTF-8"
      LC_MEASUREMENT="en_US.UTF-8"
      LC_IDENTIFICATION="en_US.UTF-8"
      LC_ALL=
    After I set `LC_ALL` to `C`, the locale outputs the following.
      LANG=en_US.UTF-8
      LC_CTYPE="C"
      LC_NUMERIC="C"
      LC_TIME="C"
      LC_COLLATE="C"
      LC_MONETARY="C"
      LC_MESSAGES="C"
      LC_PAPER="C"
      LC_NAME="C"
      LC_ADDRESS="C"
      LC_TELEPHONE="C"
      LC_MEASUREMENT="C"
      LC_IDENTIFICATION="C"
      LC_ALL=C

2.  To sort the content of the file and output the result into a file named
    `words` in my working directory. I use the following command.
    ``` shell
    cat /usr/share/dict/words | sort > words
    ```

3.  I retrieved the HTML of the assignment webpage by the following command.
    ``` shell
    wget https://web.cs.ucla.edu/classes/winter19/cs35L/assign/assign2.html
    ```
    Starting here, I will explain what each of the following command does.
    ``` shell
    # Translate each non-letter character into a newline character.
    tr -c 'A-Za-z' '[\n*]'
    # In addition to what the above command does, it squeezes repeated newline
    # characters into one occurrance, which yields a list of words.
    tr -cs 'A-Za-z' '[\n*]'
    # In addition to what the above command does, it sorts the list of words.
    tr -cs 'A-Za-z' '[\n*]' | sort
    # In addition to what the above command does, it only keep the unique words
    # in the output list.
    tr -cs 'A-Za-z' '[\n*]' | sort -u
    # In addition to what the above command does, it compares the sorted list 
    # of words with the locale word dictionary line by line, and outputs 3
    # columns: words that are unique to the list, words that are unique to the
    # dictionary file 'words', and common words.
    tr -cs 'A-Za-z' '[\n*]' | sort -u | comm - words
    # In addition to what the above command does, it only shows the words that
    # are unique to the sorted list of words from the webpage.
    tr -cs 'A-Za-z' '[\n*]' | sort -u | comm -23 - words
    ```

4.  I retrieved the HTML of page "English to Hawaiian" by the following 
    command.
    ``` shell
    wget http://mauimapp.com/moolelo/hwnwdseng.htm -O raw.html
    ```
    And then I use the following shell script to get a list of Hawaiian word.
    ``` shell
    tr "A-Z\`" "a-z\'" |
    tr -d '\n' |
    grep -o "<td>[^<]*\(<u>[^<]*</u>[^<]*\)*[^<]*</td> *</tr>" |
    sed "s/<[^>]\+>//g;s/ \+$//g" | 
    tr -s ', ' '\n' |
    grep -o "^[pk'mnwlhaeiou]\+$" |
    sort -u
    ```
    1. `tr`   treat upper case letters as if they were lower case;
              treat ` as '.
    2. `tr`   removes all the newlines in the html for easy processing.
    3. `grep` matches each `<td>` block that is right before `</tr>`, taking 
              into consideration the `<u>` block. The `-o` option makes sure 
              that outputs each match on it's own line.
    4. `sed`  remove all HTML tags and all trailing spaces with `sed`.
    5. `tr`   replace commas and spaces with newlines and squeeze consecutive 
              newlines into one.
    6. `grep` keep only the lines with Hawaiian traditional orthography.
    7. `sort` sort the result, `-u` option keeps only the unique results.

    As requested, this command checks the the spelling of Hawaiian rather than
    English.
    ``` shell
    tr "A-Z" "a-z" | tr -cs "A-Za-z\'" "[\n*]" | sort -u |
    comm -23 - hwords | wc -w
    ```

    ***** Since 'okina is part of Hawaiian, apostrophes on assign2.html are 
    kept when being checked for misspells.
    Checking the Hawaiian spelling in `assign2.html`, I get 503 "misspelled"
    Hawaiian words.
    Checking the English spelling in `assign2.html`, I get 42 "misspelled"
    English words.
    Checking the Hawaiian spelling in `hwords`, I get no misspells as
    expected.

    Outputting the misspelled English words into a file called `emisspelled`
    and the misspelled Hawaiian woords into a file called `hmisspelled`.
    We can use the following shell commands too find out if these "misspells"
    could actually mean something in the other language.
    ``` shell
    comm emisspelled hwords -23
    comm hmisspelled words -23
    ```
    In `assign2.html` there are 3 words that are misspelled as English but are
    legit Hawaiian words. Examples:
    """
    halau
    lau
    wiki
    """
    and there are 445 words that are misspelled as Hawaiian but are legit
    English words. Examples:
    """
    able
    about
    above
    """


